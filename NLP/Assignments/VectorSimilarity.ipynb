{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-slide",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "554bdab684eb16abce3c236bea52c459",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    04\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Monday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to three people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-biography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:20:04.624233Z",
     "start_time": "2024-05-06T13:20:04.613416Z"
    },
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the username of each team member into the variables. \n",
    "If you work alone please leave the other variables empty.\n",
    "'''\n",
    "member1 = 'hvu2s'\n",
    "member2 = 'anuhel2s'\n",
    "member3 = 'ksheka2s'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-relationship",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8df404bcbde65df55117c4750706470f",
     "grade": false,
     "grade_id": "VectorSimilarity_AVectorSimilarity_BVectorSimilarity_CVectorSimilarity_DVectorSimilarity_EVectorSimilarity_FVectorSimilarity_G_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Word2Vec and FastText Embeddings\n",
    "\n",
    "In this assignment we will work on Word2Vec embeddings and FastText embeddings.\n",
    "\n",
    "I prepared three dictionaries for you:\n",
    "\n",
    "- ```word2vec_yelp_vectors.pkl```: A dictionary with 300 dimensional word2vec embeddings trained on the Google News Corpus, contains only words that are present in our Yelp reviews (key is the word, value is the embedding)\n",
    "- ```fasttext_yelp_vectors.pkl```: A dictionary with 300 dimensional FastText embeddings trained on the English version of Wikipedia, contains only words that are present in our Yelp reviews (key is the word, value is the embedding)\n",
    "- ```tfidf_yelp_vectors.pkl```: A dictionary with 400 dimensional TfIdf embeddings trained on the Yelp training dataset from last assignment (key is the word, value is the embedding)\n",
    "\n",
    "In the next cell we load those into the dictionaries ```w2v_vectors```, ```ft_vectors``` and ```tfidf_vectors```.\n",
    "\n",
    "Â© Tim Metzler, Hochschule Bonn-Rhein-Sieg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sensitive-following",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:47:31.640074Z",
     "start_time": "2025-04-30T12:47:31.444339Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc1c5e46315e7059176738283b3ff597",
     "grade": false,
     "grade_id": "VectorSimilarity_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/srv/shares/NLP/embeddings/word2vec_yelp_vectors.pkl', 'rb') as f:\n",
    "    w2v_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/embeddings/fasttext_yelp_vectors.pkl', 'rb') as f:\n",
    "    ft_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/embeddings/tfidf_yelp_vectors.pkl', 'rb') as f:\n",
    "    tfidf_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/datasets/yelp/reviews_train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "with open('/srv/shares/NLP/datasets/yelp/reviews_test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "reviews = train + test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-vermont",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff8b46bb90022c6288e48aee8a4cf1b7",
     "grade": false,
     "grade_id": "VectorSimilarity_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Creating a vector model with helper functions [30 points]\n",
    "\n",
    "In the next cell we have the class ```VectorModel``` with the methods:\n",
    "\n",
    "- ```vector_size```: Returns the vector size of the model\n",
    "- ```embed```: Returns the embedding for a word. Returns None if there is no embedding present for the word\n",
    "- ```cosine_similarity```: Calculates the cosine similarity between two vectors\n",
    "- ```most_similar```: Given a word returns the ```top_n``` most similar words from the model, together with the similarity value, **sorted by similarity (descending)**. We do not want to return the word itself as the most similar one. So we only return the most similar words except for the first one.\n",
    "- ```most_similar_vec```: Given a vector returns the ```top_n``` most similar words from the model, together with the similarity value, **sorted by similarity (descending)**. Here we want to keep the most similar one.\n",
    "\n",
    "Your task is to complete these methods.\n",
    "\n",
    "Example output:\n",
    "```\n",
    "model = VectorModel(w2v_vectors)\n",
    "\n",
    "vector_good = model.embed('good')\n",
    "vector_tomato = model.embed('tomato')\n",
    "\n",
    "print(model.cosine_similarity(vector_good, vector_tomato)) # Prints: 0.05318105\n",
    "\n",
    "print(model.most_similar('tomato')) \n",
    "'''\n",
    "[('tomatoes', 0.8442263), \n",
    " ('lettuce', 0.70699364),\n",
    " ('strawberry', 0.6888598), \n",
    " ('strawberries', 0.68325955), \n",
    " ('potato', 0.67841727)]\n",
    "'''\n",
    "\n",
    "print(model.most_similar_vec(vector_good)) \n",
    "'''\n",
    "[('good', 1.0), \n",
    " ('great', 0.72915095), \n",
    " ('bad', 0.7190051), \n",
    " ('decent', 0.6837349), \n",
    " ('nice', 0.68360925)]\n",
    "'''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "honest-little",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:47:32.739078Z",
     "start_time": "2025-04-30T12:47:32.715359Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8dcf13a0fc232d23f818a43ce16c475",
     "grade": false,
     "grade_id": "VectorSimilarity_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "   \n",
    "class VectorModel:\n",
    "    \n",
    "    def __init__(self, vector_dict: Dict[str, np.ndarray]):\n",
    "        # YOUR CODE HERE\n",
    "        self.vector_dict = vector_dict \n",
    "        \n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        # YOUR CODE HERE\n",
    "        return self.vector_dict.get(word, None)\n",
    "    \n",
    "    def vector_size(self) -> int:\n",
    "        # YOUR CODE HERE\n",
    "        vector_size = len(next(iter(self.vector_dict.values())))\n",
    "        return vector_size\n",
    "    \n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        # YOUR CODE HERE\n",
    "        if vec1 is None or vec2 is None:\n",
    "            return 0\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0\n",
    "        return np.dot(vec1, vec2) / (norm1 * norm2)\n",
    "\n",
    "    def most_similar(self, word: str, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        # YOUR CODE HERE\n",
    "        vec = self.embed(word)\n",
    "        if vec is None:\n",
    "            return []\n",
    "        \n",
    "        similarities = []\n",
    "        for w, v in self.vector_dict.items():\n",
    "            if w == word:\n",
    "                continue\n",
    "            sim = self.cosine_similarity(vec, v)\n",
    "            similarities.append((w, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "        \n",
    "    def most_similar_vec(self, vec: np.ndarray, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        # YOUR CODE HERE        \n",
    "        similarities = []\n",
    "        for w, v in self.vector_dict.items():\n",
    "            sim = self.cosine_similarity(vec, v)\n",
    "            similarities.append((w, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-fault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:33:22.664079Z",
     "start_time": "2025-04-28T12:33:21.338934Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a275be8f0df6b5af65a5ce0667dd603f",
     "grade": true,
     "grade_id": "test_VectorSimilarity_A0",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wicked-norway",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc8f7068215ca1ef592778c5a4b4162a",
     "grade": false,
     "grade_id": "VectorSimilarity_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity A) [10 points]\n",
    "\n",
    "We now want to find the most similar words for a given input word for each model (Word2Vec, FastText and TfIdf).\n",
    "\n",
    "Your input words are: ```['good', 'tomato', 'restaurant', 'beer', 'wonderful']```.\n",
    "\n",
    "For each model and input word print the top three most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "improved-count",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:47:38.735487Z",
     "start_time": "2025-04-30T12:47:37.540864Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddad901f2d646951490e9fcfe8edb004",
     "grade": true,
     "grade_id": "VectorSimilarity_B",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec: Top 3 most similar words to good are [('great', 0.72915095), ('bad', 0.7190051), ('decent', 0.6837348)]\n",
      "Word2Vec: Top 3 most similar words to tomato are [('tomatoes', 0.8442263), ('lettuce', 0.70699376), ('strawberry', 0.6888598)]\n",
      "Word2Vec: Top 3 most similar words to restaurant are [('restaurants', 0.77228934), ('diner', 0.72802156), ('steakhouse', 0.72698534)]\n",
      "Word2Vec: Top 3 most similar words to beer are [('beers', 0.8409688), ('drinks', 0.66893125), ('ale', 0.63828725)]\n",
      "Word2Vec: Top 3 most similar words to wonderful are [('fantastic', 0.8047919), ('great', 0.76478696), ('fabulous', 0.7614761)]\n",
      "Word2Vec: Top 3 most similar words to dinner are [('dinners', 0.7902064), ('brunch', 0.79005134), ('breakfast', 0.7007028)]\n",
      "FastText: Top 3 most similar words to good are [('excellent', 0.7223856825801254), ('decent', 0.7202461451724537), ('bad', 0.6704173041669614)]\n",
      "FastText: Top 3 most similar words to tomato are [('eggplant', 0.7518509618329048), ('spinach', 0.7422800959168396), ('onions', 0.7328857483500281)]\n",
      "FastText: Top 3 most similar words to restaurant are [('restaurants', 0.8384667264823358), ('bistro', 0.7845601578005464), ('bakery', 0.7155727705943096)]\n",
      "FastText: Top 3 most similar words to beer are [('beers', 0.7944971406865431), ('brewed', 0.7929903321082489), ('brewery', 0.7520785637582763)]\n",
      "FastText: Top 3 most similar words to wonderful are [('lovely', 0.6808215868395576), ('fascinating', 0.6745727685452472), ('amazing', 0.6457084279396067)]\n",
      "FastText: Top 3 most similar words to dinner are [('dinners', 0.8463012295986414), ('brunch', 0.709906334988423), ('meal', 0.6719537670739056)]\n",
      "TFIDF: Top 3 most similar words to good are [('the', 0.6199071144484399), ('a', 0.6170194328254505), ('and', 0.6121212998064655)]\n",
      "TFIDF: Top 3 most similar words to tomato are [('provolone', 0.7071067811865476), ('ceasar', 0.7071067811865475), ('cheesesteak', 0.7071067811865475)]\n",
      "TFIDF: Top 3 most similar words to restaurant are [('held', 0.46881558553353003), ('patrons', 0.46178340768084236), ('we', 0.4308073684733683)]\n",
      "TFIDF: Top 3 most similar words to beer are [('tap', 0.6326077388711026), ('beers', 0.5132564299212761), ('blowingly', 0.4746774221449982)]\n",
      "TFIDF: Top 3 most similar words to wonderful are [('truffle', 0.6264995084522798), ('accident', 0.5432509277196604), ('equally', 0.5432509277196604)]\n",
      "TFIDF: Top 3 most similar words to dinner are [('slaw', 0.4842078303350308), ('timely', 0.4038564890706335), ('plates', 0.3828945646253459)]\n"
     ]
    }
   ],
   "source": [
    "input_words = ['good', 'tomato', 'restaurant', 'beer', 'wonderful', 'dinner']\n",
    "\n",
    "# Word2Vec\n",
    "w2v_model = VectorModel(vector_dict = w2v_vectors)\n",
    "for input_word in input_words:\n",
    "    similar_words = w2v_model.most_similar(input_word, 3)\n",
    "    print(f\"Word2Vec: Top 3 most similar words to {input_word} are {similar_words}\")\n",
    "\n",
    "# FastText    \n",
    "ft_model = VectorModel(vector_dict = ft_vectors)\n",
    "for input_word in input_words:\n",
    "    similar_words = ft_model.most_similar(input_word, 3)\n",
    "    print(f\"FastText: Top 3 most similar words to {input_word} are {similar_words}\")\n",
    "    \n",
    "# TFIDF  \n",
    "tfidf_model = VectorModel(vector_dict = tfidf_vectors)\n",
    "for input_word in input_words:\n",
    "    similar_words = tfidf_model.most_similar(input_word, 3)\n",
    "    print(f\"TFIDF: Top 3 most similar words to {input_word} are {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-ultimate",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea42db99d9e4f8eceab5247a1e742d2f",
     "grade": false,
     "grade_id": "VectorSimilarity_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity B) [10 points]\n",
    "\n",
    "Comment on the output from the previous task. Let us look at the output for the word ```wonderful```. How do the models differ for this word? Can you reason why the TfIdf model shows so different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-aviation",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "560f030fbc0ebf9c093d42f0af3d37c5",
     "grade": true,
     "grade_id": "VectorSimilarity_C",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "W2V model outputs: fantastic, great and fabulous. Meanwhile FT model gives out: lovely, fascinating and amazing. Given this results alone I feel like both performances are valid. On the other hand, TFIDF model gave a completely bad outputs. Maybe because vectors represented through TFIDF have no relationship to each other even for similar words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-answer",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0577bd13c5771b100b73b4d013022259",
     "grade": false,
     "grade_id": "VectorSimilarity_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity C) [10 points]\n",
    "\n",
    "Instead of just finding the most similar word to a single word, we can also find the most similar word given a list of positive and negative words.\n",
    "\n",
    "For this we just sum up the positive and negative words into a single vector by calculating a weighted mean. For this we multiply each positive word with a factor of $+1$ and each negative word with a factor of $-1$. Then we get the most similar words to that vector.\n",
    "\n",
    "You are given the following examples:\n",
    "\n",
    "```\n",
    "inputs = [\n",
    "    {\n",
    "        'positive': ['good', 'wonderful'],\n",
    "        'negative': ['bad']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['tomato', 'lettuce'],\n",
    "        'negative': ['strawberry', 'salad']\n",
    "    }    \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "advanced-shelf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:47:47.009889Z",
     "start_time": "2025-04-30T12:47:46.543556Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c11d7cc8e7d41bae0b33df45bc3e5433",
     "grade": true,
     "grade_id": "VectorSimilarity_D",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec: Top 3 most similar words to {'positive': ['good', 'wonderful'], 'negative': ['bad']} are [('terrible', 0.6828612), ('horrible', 0.67025983), ('lousy', 0.66476405)]\n",
      "Word2Vec: Top 3 most similar words to {'positive': ['tomato', 'lettuce'], 'negative': ['strawberry', 'fruit']} are [('fruits', 0.77371895), ('berries', 0.6854092), ('mango', 0.6631807)]\n",
      "Word2Vec: Top 3 most similar words to {'positive': ['ceasar', 'chicken'], 'negative': []} are [('meat', 0.67991304), ('pork', 0.6541998), ('turkey', 0.6282519)]\n",
      "FastText: Top 3 most similar words to {'positive': ['good', 'wonderful'], 'negative': ['bad']} are [('nasty', 0.6049038343820987), ('lousy', 0.5805090777602171), ('awful', 0.574274257647221)]\n",
      "FastText: Top 3 most similar words to {'positive': ['tomato', 'lettuce'], 'negative': ['strawberry', 'fruit']} are [('fruits', 0.8618268613306459), ('berries', 0.7313970644025728), ('edible', 0.6641486013579591)]\n",
      "FastText: Top 3 most similar words to {'positive': ['ceasar', 'chicken'], 'negative': []} are [('pork', 0.7396497859521203), ('beef', 0.7368674943230636), ('patties', 0.7110088918383924)]\n"
     ]
    }
   ],
   "source": [
    "class AnalogyVectorModel(VectorModel):\n",
    "    \n",
    "    def __init__(self, vector_dict: Dict[str, np.ndarray]):\n",
    "        # YOUR CODE HERE\n",
    "        super().__init__(vector_dict) \n",
    "        \n",
    "    \n",
    "    def most_similar_analogy(self, analogy_words: dict, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        # YOUR CODE HERE\n",
    "        vec_sum = np.zeros(super().vector_size())\n",
    "        \n",
    "        for word in analogy_words[\"positive\"]:\n",
    "            vec = self.embed(word)\n",
    "            if vec is not None:\n",
    "                vec_sum += vec\n",
    "        for word in analogy_words[\"negative\"]:\n",
    "            vec = self.embed(word)\n",
    "            if vec is not None:\n",
    "                vec_sum -= vec\n",
    "        word_list = analogy_words[\"positive\"] + analogy_words[\"negative\"]\n",
    "            \n",
    "        \n",
    "        similarities = []\n",
    "        for w, v in self.vector_dict.items():\n",
    "            if w in word_list:\n",
    "                continue\n",
    "            sim = super().cosine_similarity(vec, v)\n",
    "            similarities.append((w, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "inputs = [\n",
    "    {\n",
    "        'positive': ['good', 'wonderful'],\n",
    "        'negative': ['bad']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['tomato', 'lettuce'],\n",
    "        'negative': ['strawberry', 'fruit']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['ceasar', 'chicken'],\n",
    "        'negative': []\n",
    "    }    \n",
    "]\n",
    "# YOUR CODE HERE\n",
    "w2v_analogy_model = AnalogyVectorModel(w2v_vectors)\n",
    "for input_dict in inputs:\n",
    "    similar_words = w2v_analogy_model.most_similar_analogy(input_dict, 3)\n",
    "    print(f\"Word2Vec: Top 3 most similar words to {input_dict} are {similar_words}\")\n",
    "    \n",
    "ft_analogy_model = AnalogyVectorModel(ft_vectors)\n",
    "for input_dict in inputs:\n",
    "    similar_words = ft_analogy_model.most_similar_analogy(input_dict, 3)\n",
    "    print(f\"FastText: Top 3 most similar words to {input_dict} are {similar_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-reset",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aeb5076326c30f2b229061cc1d1dc58",
     "grade": false,
     "grade_id": "VectorSimilarity_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity D) [15 points]\n",
    "\n",
    "We can use our model to find out which word does not match given a list of words.\n",
    "\n",
    "For this we build the mean vector of all embeddings in the list.  \n",
    "Then we calculate the cosine similarity between the mean and all those vectors.\n",
    "\n",
    "The word that does not match is then the word with the lowest cosine similarity to the mean.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "model = VectorModel(w2v_vectors)\n",
    "doesnt_match(model, ['potato', 'tomato', 'beer']) # -> 'beer'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "featured-ballot",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:47:59.587498Z",
     "start_time": "2025-04-30T12:47:59.569794Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07a2a155084a8433ed5f6a649b4b9c3b",
     "grade": false,
     "grade_id": "VectorSimilarity_E",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vegetable'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doesnt_match(model, words):\n",
    "    # YOUR CODE HERE\n",
    "    vec_sum = np.zeros(model.vector_size())\n",
    "    embed_list = []\n",
    "    for word in words:\n",
    "        word_embed = model.embed(word)\n",
    "        embed_list.append((word, word_embed))\n",
    "        vec_sum += word_embed\n",
    "    vec_mean = vec_sum/len(words)\n",
    "    \n",
    "    similarity_scores = []\n",
    "    for embeb in embed_list:\n",
    "        similarity_score = model.cosine_similarity(vec_mean, embeb[1])\n",
    "        similarity_scores.append((embeb[0], similarity_score))\n",
    "    min_tuple = min(similarity_scores, key=lambda x: x[1])\n",
    "    return min_tuple[0]\n",
    "\n",
    "    \n",
    "doesnt_match(VectorModel(w2v_vectors), ['vegetable', 'strawberry', 'tomato', 'lettuce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-conclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:33:32.562401Z",
     "start_time": "2025-04-28T12:33:32.422819Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b0ba5c75465a8e985326f69ef7da279",
     "grade": true,
     "grade_id": "test_VectorSimilarity_E0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "responsible-parent",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d141ef5c3f341644d3a756404c50be53",
     "grade": false,
     "grade_id": "VectorSimilarity_F_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Document Embeddings A) [15 points]\n",
    "\n",
    "Now we want to create document embeddings similar to the last assignment. For this you are given the function ```bagOfWords```. In the context of Word2Vec and FastText embeddings this is also called ```SOWE``` for sum of word embeddings.\n",
    "\n",
    "Take the yelp reviews (```reviews```) and create a dictionary containing the document id as a key and the document embedding as a value.\n",
    "\n",
    "Create the document embeddings from the Word2Vec, FastText and TfIdf embeddings.\n",
    "\n",
    "Store these in the variables ```ft_doc_embeddings```, ```w2v_doc_embeddings``` and ```tfidf_doc_embeddings```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "serial-forum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:57:54.865064Z",
     "start_time": "2025-04-30T12:57:54.463481Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e69f73310a129b0adbface4154bf02c5",
     "grade": false,
     "grade_id": "VectorSimilarity_F",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords(model, doc: List[str]) -> np.ndarray:\n",
    "    '''\n",
    "    Create a document embedding using the bag of words approach\n",
    "    \n",
    "    Args:\n",
    "        model     -- The embedding model to use\n",
    "        doc       -- A document as a list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        embedding -- The embedding for the document as a single vector \n",
    "    '''\n",
    "    embeddings = [np.zeros(model.vector_size())]\n",
    "    n_tokens = 0\n",
    "    for token in doc:\n",
    "        embedding = model.embed(token)\n",
    "        if embedding is not None:\n",
    "            n_tokens += 1\n",
    "            embeddings.append(embedding)\n",
    "    if n_tokens > 0:\n",
    "        return sum(embeddings)/n_tokens\n",
    "    return sum(embeddings)\n",
    "\n",
    "\n",
    "ft_doc_embeddings = dict()\n",
    "w2v_doc_embeddings = dict()\n",
    "tfidf_doc_embeddings = dict()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for review in reviews:\n",
    "    ft_doc_embeddings[review[\"id\"]] = bagOfWords(ft_model, review[\"tokens\"])\n",
    "    w2v_doc_embeddings[review[\"id\"]] = bagOfWords(w2v_model, review[\"tokens\"])\n",
    "    tfidf_doc_embeddings[review[\"id\"]] = bagOfWords(tfidf_model, review[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-forwarding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:33:35.204692Z",
     "start_time": "2025-04-28T12:33:34.966597Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c1d7d5c9497b1ebaf9a44d10aadb1a4",
     "grade": true,
     "grade_id": "test_VectorSimilarity_F0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "advisory-slovak",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ea86310ebff375544ce2e08fff4cff9",
     "grade": false,
     "grade_id": "VectorSimilarity_G_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Document Embeddings B) [10 points]\n",
    "\n",
    "Create a vector model from each of the document embedding dictionaries. Call these ```model_w2v_doc```, ```model_ft_doc``` and ```model_tfidf_doc```.\n",
    "\n",
    "Now find the most similar document (```top_n=1```) for document $438$ with each of these models. Use the method `most_similar`. For example `model.most_similar(438)`.\n",
    "\n",
    "Print the text for each of the most similar reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "persistent-fairy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T13:06:54.586662Z",
     "start_time": "2025-04-30T13:06:54.569387Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71ccd4569a5255386cafbbbc1cc0b510",
     "grade": true,
     "grade_id": "VectorSimilarity_G",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source document:\n",
      "Absolutely ridiculously amazing! Chicken Tikka masala was perfect. Best I've ever had!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First find the text for review 438\n",
    "def find_doc(doc_id, reviews):\n",
    "    for review in reviews:\n",
    "        if review['id'] == doc_id:\n",
    "            return review['text']\n",
    "    \n",
    "doc_id = 438\n",
    "\n",
    "# Print it\n",
    "print('Source document:')\n",
    "print(find_doc(doc_id, reviews))\n",
    "\n",
    "# Create the models\n",
    "model_w2v_doc = None\n",
    "model_ft_doc = None\n",
    "model_tfidf_doc = None\n",
    "\n",
    "model_w2v_doc = VectorModel(vector_dict = w2v_doc_embeddings)\n",
    "similar_doc = model_w2v_doc.most_similar(find_doc(doc_id, reviews), 1)\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d74a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
