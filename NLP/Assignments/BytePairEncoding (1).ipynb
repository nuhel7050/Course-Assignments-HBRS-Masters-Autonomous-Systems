{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-slide",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fee7dcd8dfea9fad6ceec13a09f43eee",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    Assignment 01\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Monday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to three people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-biography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T09:38:25.661792Z",
     "start_time": "2025-04-01T09:38:25.631312Z"
    },
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the username of each team member into the variables. \n",
    "If you work alone please leave the second variable empty.\n",
    "'''\n",
    "member1 = 'hvu2s'\n",
    "member2 = 'anuhel2s'\n",
    "member3 = 'ksheka2s'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b44acb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a71fd5a7ffb0b58b00993e75290a656",
     "grade": false,
     "grade_id": "BytePairEncoding_ABytePairEncoding_BBytePairEncoding_CBytePairEncoding_DBytePairEncoding_EBytePairEncoding_FBytePairEncoding_G_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Byte Pair Encoding\n",
    "\n",
    "We want to implement BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a834fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0944bf41e5684886879b9a5f916a1712",
     "grade": false,
     "grade_id": "BytePairEncoding_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding A) [10 points]\n",
    "\n",
    "First we want to do pre-tokenization using white spaces.\n",
    "\n",
    "Please complete the function `pretokenize` below. This takes a list of sentences or documents and returns a list of tokenized sentences or documents. Look at the example in the docstring for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab080389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T08:42:59.762826Z",
     "start_time": "2025-04-13T08:42:59.751402Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d89e96c662b6e634e63447bf51e1ba1b",
     "grade": false,
     "grade_id": "BytePairEncoding_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'an', 'example', 'sentence'],\n",
       " ['Another', 'sentence'],\n",
       " ['The', 'final', 'sentence.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def pretokenize(sentences: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenizes a list of sentences into a list of lists of tokens using white spaces.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): List of sentences to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: List of lists of tokens, where each inner list represents\n",
    "                         the tokens of a single sentence.\n",
    "    Example:\n",
    "        >>> sentences = [\"Hello world\", \"This is a test!\"]\n",
    "        >>> pretokenize(sentences)\n",
    "        [['Hello', 'world'], ['This', 'is', 'a', 'test!']]\n",
    "    \"\"\"\n",
    "    if len(sentences) == 0:\n",
    "        raise ValueError(\"Sentence list should not be empty\")\n",
    "    else:\n",
    "        tokenized_list = []\n",
    "        for sentence in sentences:\n",
    "            tokenized = sentence.strip().split()\n",
    "            tokenized_list.append(tokenized)\n",
    "    return tokenized_list\n",
    "    \n",
    "example_sentences = [\n",
    "    \"This is an  example sentence\",\n",
    "    \"Another sentence\",\n",
    "    \"The final sentence.\"\n",
    "]\n",
    "\n",
    "tokenized = pretokenize(example_sentences)\n",
    "\"\"\"\n",
    "Expected Output:\n",
    "[['This', 'is', 'an', 'example', 'sentence'],\n",
    " ['Another', 'sentence'],\n",
    " ['The', 'final', 'sentence.']]\n",
    "\"\"\"\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2d01b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T09:38:25.820747Z",
     "start_time": "2025-04-01T09:38:25.733639Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b80a983de1ea49369d28cb654b742017",
     "grade": true,
     "grade_id": "test_BytePairEncoding_A0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a73b7e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99c8c7a90be02c63999d2e2c554059c4",
     "grade": false,
     "grade_id": "BytePairEncoding_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding B) [10 points]\n",
    "\n",
    "For BPE we first need an initial vocabulary. The input is a pretokenized list of sentences / documents.\n",
    "\n",
    "The output should be a set of characters present in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaea461c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T08:43:01.406380Z",
     "start_time": "2025-04-13T08:43:01.390166Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16461814fb5df1c71713134281739b77",
     "grade": false,
     "grade_id": "BytePairEncoding_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T', 'a', 'd', 'e', 'h', 'i', 'l', 'o', 'r', 's', 't', 'w'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "def build_initial_vocabulary(corpus: List[List[str]]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Build the initial vocabulary from a corpus of tokenized sentences.\n",
    "\n",
    "    Args:\n",
    "        corpus (List[List[str]]): A list of tokenized sentences, where each sentence\n",
    "            is represented as a list of strings (tokens).\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: A set containing all unique characters from all tokens in the corpus.\n",
    "\n",
    "    Example:\n",
    "        >>> corpus = [['hello', 'world'], ['This', 'is', 'a', 'test']]\n",
    "        >>> build_initial_vocabulary(corpus)\n",
    "        {'T', 'a', 'd', 'e', 'h', 'i', 'l', 'o', 'r', 's', 't', 'w'}\n",
    "        # Note: Order may vary since sets are unordered\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    for tokenized_list in corpus:\n",
    "        for token in tokenized_list:\n",
    "            vocabulary = vocabulary | set(token)\n",
    "    return vocabulary\n",
    "    \n",
    "vocabulary = build_initial_vocabulary(pretokenize([\"hello world\", \"This is a test\"]))\n",
    "\"\"\"\n",
    "Expected Output:\n",
    "{'T', 'a', 'd', 'e', 'h', 'i', 'l', 'o', 'r', 's', 't', 'w'}\n",
    "# Note: Order may vary since sets are unordered\n",
    "\"\"\"\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058738b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T09:38:25.914533Z",
     "start_time": "2025-04-01T09:38:25.849936Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9c4d770a17894a922c0f23e9a3e9030",
     "grade": true,
     "grade_id": "test_BytePairEncoding_B0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab5b925",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0644bf14c439654cc79b9717eeb3c62",
     "grade": false,
     "grade_id": "BytePairEncoding_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding C) [10 points]\n",
    "\n",
    "\n",
    "Now we want to build our dictionary for the split tokens. Complete the function `get_splits` below. Look at the example in the docstring!\n",
    "\n",
    "Make sure to add the end of word symbol (`</w>`) to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f5e464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T08:43:02.453849Z",
     "start_time": "2025-04-13T08:43:02.436583Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "501a30353eeb5c4977819569515bf016",
     "grade": false,
     "grade_id": "BytePairEncoding_C",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
       " ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def get_splits(corpus: List[List[str]]) -> Dict[Tuple[str], int]:\n",
    "    \"\"\"\n",
    "    Get subword splits of tokens in a corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus (List[List[str]]): A list of sentences where each sentence is represented\n",
    "            as a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[str], int]: A dictionary where keys are tuples representing subword splits\n",
    "            and values are the counts of occurrences of those splits in the corpus.\n",
    "\n",
    "    Example:\n",
    "        >>> corpus = [['apple', 'banana', 'apple'], ['apple']]\n",
    "        >>> get_splits(corpus)\n",
    "        {('a', 'p', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\n",
    "    \"\"\"\n",
    "    split_tokens = []\n",
    "    for tokenized_list in corpus:\n",
    "        for token in tokenized_list:\n",
    "            counter = 1\n",
    "            split_token = tuple(token) + (\"</w>\",)\n",
    "            split_tokens.append(split_token)\n",
    "    return dict(Counter(split_tokens))\n",
    "    \n",
    "get_splits(pretokenize([\"apple banana apple\", \"apple\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c2643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T09:38:26.031814Z",
     "start_time": "2025-04-01T09:38:25.980930Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "368952d06bdb7546506c7d8b7b2a5452",
     "grade": true,
     "grade_id": "test_BytePairEncoding_C0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c86d3f7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d25120cd363fa7b55da4974e989d2cc4",
     "grade": false,
     "grade_id": "BytePairEncoding_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding D) [10 points]\n",
    "\n",
    "In the next step we want to find the most common pair from a splits dictionary.\n",
    "\n",
    "Complete the function `find_most_frequent_pair` which returns the most frequent pair alongside its count (e.g. `(('a', 'n'), 2)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae731b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T08:43:03.665549Z",
     "start_time": "2025-04-13T08:43:03.645186Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96d650a1d40df77f936f3b1978667db8",
     "grade": false,
     "grade_id": "BytePairEncoding_D",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('l', 'e'), 4)\n",
      "(('a', '</w>'), 2)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "def find_most_frequent_pair(splits: Dict[Tuple[str], int]) -> Tuple[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Find the most frequent pair of characters from a dictionary of split words along with its count.\n",
    "\n",
    "    Args:\n",
    "        splits (Dict[Tuple[str], int]): A dictionary where keys are tuples of split words and values \n",
    "            are their counts.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tuple[str, str], int]: A tuple containing the most frequent pair of characters and its count.\n",
    "\n",
    "    Example:\n",
    "        >>> splits = {('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
    "                      ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1,\n",
    "                      ('l', 'e', 'a', 'd', '</w>'): 1}\n",
    "        >>> find_most_frequent_pair(splits)\n",
    "        (('l', 'e'), 4) \n",
    "        # Explanation: 'l' and 'e' appear in 'apple' and 'lead'. \n",
    "        # The word 'apple' appears 3x and the word 'lead' 1x.\n",
    "    \"\"\"\n",
    "    adjacent_pair_list = []\n",
    "    for splitted_token, count in splits.items():\n",
    "        for i in range(len(splitted_token)-1):\n",
    "            adjacent_pair_list.extend([(splitted_token[i], splitted_token[i+1])]*count)\n",
    "    adjacent_pair_counter = Counter(adjacent_pair_list)\n",
    "    return adjacent_pair_counter.most_common(1)[0]\n",
    "    \n",
    "print(find_most_frequent_pair(get_splits(pretokenize([\"apple banana apple\", \"apple lead\"]))))\n",
    "\n",
    "print(find_most_frequent_pair(get_splits(pretokenize([\"a\", \"a b\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5958dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T09:38:26.173999Z",
     "start_time": "2025-04-01T09:38:26.122337Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8ddd02fa5547d8270eaba4c542572f",
     "grade": true,
     "grade_id": "test_BytePairEncoding_D0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae59af20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06c946410862f0b18d53883b2ba0930f",
     "grade": false,
     "grade_id": "BytePairEncoding_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding E) [15 points]\n",
    "\n",
    "Now write a function that takes a pair and the splits and merges all occurences of the pair in the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7016d5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T08:43:04.867546Z",
     "start_time": "2025-04-13T08:43:04.843621Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbea57e97fecc48f16e8cbdfc6873416",
     "grade": false,
     "grade_id": "BytePairEncoding_E",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('ap', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_split(split: Tuple[str], pair: Tuple[str, str]):\n",
    "    \"\"\"\n",
    "    Merge a split tuple if it contains the given pair.\n",
    "    This function merges **all occurrences** of the pair in the split.\n",
    "\n",
    "    Args:\n",
    "        split (Tuple[str]): The split tuple to merge.\n",
    "        pair (Tuple[str, str]): The pair to merge.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str]: The merged split tuple.\n",
    "        \n",
    "    Example:\n",
    "        >>> merge_split(split=('b', 'a', 'n', 'a', 'n', 'a', '</w>'), pair=('a', 'n'))\n",
    "        # Returns:\n",
    "        ('b', 'an', 'an', 'a', '</w>')\n",
    "        \n",
    "        >>> merge_split(split=('b', 'an', 'an', 'a', '</w>'), pair=('an', 'an'))\n",
    "        # Returns:\n",
    "        ('b', 'anan', 'a', '</w>')\n",
    "    \"\"\"\n",
    "    merge_split = []\n",
    "    i = 0\n",
    "    while i < len(split):\n",
    "        if i < len(split) - 1 and split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "            merge_split.append(split[i] + split[i + 1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merge_split.append(split[i])\n",
    "            i += 1\n",
    "    return tuple(merge_split)\n",
    "\n",
    "def merge_splits(splits: Dict[Tuple[str], int], pair: Tuple[str, str]):\n",
    "    \"\"\"\n",
    "    Merge all split tuples in a dictionary that contain the given pair.\n",
    "\n",
    "    Args:\n",
    "        splits (Dict[Tuple[str], int]): A dictionary of split tuples and their counts.\n",
    "        pair (Tuple[str, str]): The pair to merge.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[str], int]: A dictionary with merged split tuples and their counts.\n",
    "        \n",
    "    Example:\n",
    "        >>> splits = {\n",
    "            ('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
    "            ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1\n",
    "        }\n",
    "        >>> merge_splits(splits, ('a', 'n'))\n",
    "        # Returns this:\n",
    "        {\n",
    "            ('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
    "            ('b', 'an', 'an', 'a', '</w>'): 1\n",
    "        }\n",
    "    \"\"\"\n",
    "    merged_splits = {}\n",
    "    for split, count in splits.items():\n",
    "        merged_split = merge_split(split, pair)\n",
    "        merged_splits[merged_split] = count\n",
    "    return merged_splits\n",
    "    \n",
    "splits = get_splits(pretokenize([\"apple banana apple\", \"apple\"]))\n",
    "\n",
    "most_frequent_pair, count = find_most_frequent_pair(splits)\n",
    "\n",
    "merge_splits(splits, most_frequent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590817c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13299191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T09:38:26.293679Z",
     "start_time": "2025-04-01T09:38:26.230568Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbb68404c1e9e7653d07987d67aa40ff",
     "grade": true,
     "grade_id": "test_BytePairEncoding_E0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "063b44cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f91d34ba89c586601cd35bd4b0139502",
     "grade": false,
     "grade_id": "BytePairEncoding_F_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding F) [40 points]\n",
    "\n",
    "Now let us put this all together into a single class. Complete the methods `train`, `encode` and `decode`.\n",
    "\n",
    "- `train` will learn the vocabulary and a list of merged pairs to use for encoding / tokenizing.\n",
    "- `encode` will tokenize a list of strings using the merge rules by applying them in order\n",
    "- `decode` will take a BPE encoded list of lists and merge subwords\n",
    "\n",
    "Look at the examples in the docstrings for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9684a22d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:35:42.166520Z",
     "start_time": "2025-04-13T11:35:42.126310Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "304adc6ed5f7dbeed18979b80f6c653b",
     "grade": false,
     "grade_id": "BytePairEncoding_F",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary is:\n",
      "{'n', 'o', 'r', 'l', 'st', 't', 'lowe', 'newe', 'lowest</w>', 'e', 'lo', 'w', 'newer</w>', 'r</w>', 'w</w>', 'we', 's', 'ne', 'st</w>', 'lower</w>'}\n",
      "We learned the following merge pairs:\n",
      "[('w', 'e'), ('l', 'o'), ('lo', 'we'), ('r', '</w>'), ('n', 'e'), ('s', 't'), ('st', '</w>'), ('lowe', 'r</w>'), ('ne', 'we'), ('w', '</w>'), ('lowe', 'st</w>'), ('newe', 'r</w>')]\n",
      "The encoded corpus:\n",
      "[['lowest</w>', 'lower</w>', 'newer</w>', 'newe', 'st</w>'], ['lo', 'w</w>', 'lower</w>', 'ne', 'w</w>']]\n",
      "The decoded corpus:\n",
      "[['lowest', 'lower', 'newer', 'newest'], ['low', 'lower', 'new']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nExpected decoded corpus:\\n[\\n    ['lowest', 'lower', 'newer', 'newest'], \\n    ['low', 'lower', 'new']\\n]\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    Byte-Pair Encoding (BPE) Tokenizer.\n",
    "    \n",
    "    This tokenizer learns a vocabulary and encodes/decodes text using the Byte-Pair Encoding algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BPETokenizer.\n",
    "        \"\"\"\n",
    "        self.vocab: set = set() # Vocabulary learned from the corpus. Initially just characters, later also joined characters.\n",
    "        self.end_of_word: str = \"</w>\" # End of word symbol\n",
    "        self.merge_pairs: List[Tuple[str, str]] = [] # List of merge pairs learned from the corpus\n",
    "        \n",
    "    def train(self, corpus: List[str], max_vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Train the tokenizer on a given corpus.\n",
    "        This method pretokenizes the corpus, builds an initial vocabulary, and applies BPE\n",
    "        to learn merge pairs until the vocabulary reaches the max_vocab_size.\n",
    "        \n",
    "        Steps:\n",
    "        1. Pretokenize\n",
    "        2. Build initial vocab\n",
    "        3. Get initial splits\n",
    "        4. Until we reach the max_vocab_size or do not have a most frequent pair DO:\n",
    "            4.1 Find most frequent pair\n",
    "            4.2 Update your splits by using the merge_splits function\n",
    "            4.3 Update vocab\n",
    "            4.4 Update merge pairs\n",
    "            \n",
    "        Hint:\n",
    "        Make sure to update correctly everytime you find a new most frequent pair:\n",
    "        Assume your most frequent pair is (\"ba\", \"na\"). Then you add \"bana\" to your self.vocab\n",
    "        and (\"ba\", \"na\") to self.merge_pairs\n",
    "           \n",
    "\n",
    "        Args:\n",
    "            corpus (List[str]): The corpus of text for training.\n",
    "            max_vocab_size (int): The maximum size of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            None: In the end your tokenizer will have an updated vocabulary and merge pairs.\n",
    "            \n",
    "        Example:\n",
    "        >>> corpus = [\n",
    "            \"lowest lower newer newest\",\n",
    "            \"low lower new\"\n",
    "        ]\n",
    "        >>> tokenizer.train(corpus, max_vocab_size=20)\n",
    "        \"\"\"\n",
    "        # 1. Pretokenize\n",
    "        tokenized_list = pretokenize(corpus)\n",
    "        # 2. Build initial vocabulary\n",
    "        self.vocab = build_initial_vocabulary(tokenized_list)\n",
    "        # 3. Get init split\n",
    "        splits = get_splits(tokenized_list)\n",
    "        # 4. Merging\n",
    "        while True:\n",
    "            # Find most frequent pair\n",
    "            most_frequent_pair, _ = find_most_frequent_pair(splits)\n",
    "            # Update split\n",
    "            splits = merge_splits(splits, most_frequent_pair)\n",
    "            # Update vocab\n",
    "            self.vocab.add(f\"{most_frequent_pair[0]}{most_frequent_pair[1]}\")\n",
    "            # Update merge pairs\n",
    "            self.merge_pairs.append(most_frequent_pair)\n",
    "            # Check vocab size\n",
    "            if len(self.vocab) >= max_vocab_size:\n",
    "                break\n",
    "                                     \n",
    "        \n",
    "    def encode(self, corpus: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Encode / Tokenize a corpus of text using the learned vocabulary and merge pairs.\n",
    "        This method applies the learned Byte Pair Encoding (BPE) algorithm to a given corpus.\n",
    "        It splits each word in the input corpus, adds the special end-of-word token `</w>`,\n",
    "        and applies the learned merge pairs in order to convert words into subword units.\n",
    "\n",
    "        Args:\n",
    "            corpus (List[str]): The corpus of text to be encoded.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: The encoded corpus.\n",
    "        \n",
    "        Example:\n",
    "        >>> corpus = [\n",
    "            \"lowest lower newer newest\",\n",
    "            \"low lower new\"\n",
    "        ]\n",
    "        >>> tokenizer.train(corpus, max_vocab_size=20)\n",
    "        >>> tokenizer.encode(corpus)\n",
    "        [['lowest</w>', 'lower</w>', 'newer</w>', 'newe', 'st</w>'],\n",
    "         ['lo', 'w</w>', 'lower</w>', 'ne', 'w</w>']]\n",
    "        \n",
    "        \"\"\"\n",
    "        encoded_corpus = []\n",
    "        for sentence in corpus:\n",
    "            tokens = sentence.strip().split()            \n",
    "            token_list = []\n",
    "            for token in tokens:\n",
    "                merged = tuple(token)\n",
    "                merged += (\"</w>\",)\n",
    "                for learned_pair in self.merge_pairs:\n",
    "                    merged = merge_split(merged, learned_pair)\n",
    "                token_list.extend(list(merged))\n",
    "            encoded_corpus.append(token_list)       \n",
    "        return encoded_corpus\n",
    "                \n",
    "        \n",
    "        \n",
    "    def decode(self, tokenized: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Decode a corpus of tokenized text.\n",
    "\n",
    "        Args:\n",
    "            tokenized (List[List[str]]): The tokenized text to be decoded.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: The decoded text.\n",
    "            \n",
    "        Example:\n",
    "        >>> corpus = [\n",
    "            \"lowest lower newer newest\",\n",
    "            \"low lower new\"\n",
    "        ]\n",
    "        >>> tokenizer.train(corpus, max_vocab_size=20)\n",
    "        >>> tokenizer.decode([['lowest</w>', 'lower</w>', 'newer</w>', 'newe', 'st</w>'],\n",
    "                              ['lo', 'w</w>', 'lower</w>', 'ne', 'w</w>']])\n",
    "        [['lowest', 'lower', 'newer', 'newest'], ['low', 'lower', 'new']]                              \n",
    "        \"\"\"\n",
    "        decoded_corpus = []\n",
    "        for sentence in tokenized:\n",
    "            decoded_sentence = []\n",
    "            current_word = \"\"\n",
    "            for token in sentence:\n",
    "                if token.endswith(self.end_of_word):\n",
    "                    current_word += token.replace(self.end_of_word, \"\")\n",
    "                    decoded_sentence.append(current_word)\n",
    "                    current_word = \"\"\n",
    "                else:\n",
    "                    current_word += token\n",
    "            decoded_corpus.append(decoded_sentence)\n",
    "        return decoded_corpus\n",
    "            \n",
    "                    \n",
    "        \n",
    "corpus = [\n",
    "    \"lowest lower newer newest\",\n",
    "    \"low lower new\"\n",
    "]\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(corpus, 20)\n",
    "\n",
    "print(\"Vocabulary is:\")\n",
    "print(tokenizer.vocab)\n",
    "\"\"\"\n",
    "Expected vocabulary:\n",
    "{'e', 'l', 'lo', 'lowe', 'lower</w>', 'lowest</w>', 'n',\n",
    " 'ne', 'newe', 'newer</w>', 'o', 'r', 'r</w>', 's', 'st',\n",
    " 'st</w>', 't', 'w', 'w</w>', 'we'}\n",
    "\"\"\"\n",
    "\n",
    "print(\"We learned the following merge pairs:\")\n",
    "\"\"\"\n",
    "Expected merge pairs:\n",
    "[\n",
    "    ('w', 'e'), ('l', 'o'), ('lo', 'we'), ('r', '</w>'),\n",
    "    ('n', 'e'), ('s', 't'), ('st', '</w>'), ('lowe', 'r</w>'),\n",
    "    ('ne', 'we'), ('w', '</w>'), ('lowe', 'st</w>'), ('newe', 'r</w>')\n",
    "]\n",
    "\"\"\"\n",
    "print(tokenizer.merge_pairs)\n",
    "\n",
    "encoded = tokenizer.encode(corpus)\n",
    "print(\"The encoded corpus:\")\n",
    "print(encoded)\n",
    "\"\"\"\n",
    "Expected encoded corpus:\n",
    "[\n",
    "    ['lowest</w>', 'lower</w>', 'newer</w>', 'newe', 'st</w>'], \n",
    "    ['lo', 'w</w>', 'lower</w>', 'ne', 'w</w>']\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"The decoded corpus:\")\n",
    "print(decoded)\n",
    "\"\"\"\n",
    "Expected decoded corpus:\n",
    "[\n",
    "    ['lowest', 'lower', 'newer', 'newest'], \n",
    "    ['low', 'lower', 'new']\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37a5fe7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:35:57.004704Z",
     "start_time": "2025-04-13T11:35:56.994707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w', 'e'),\n",
       " ('l', 'o'),\n",
       " ('lo', 'we'),\n",
       " ('r', '</w>'),\n",
       " ('n', 'e'),\n",
       " ('s', 't'),\n",
       " ('st', '</w>'),\n",
       " ('lowe', 'r</w>'),\n",
       " ('ne', 'we'),\n",
       " ('w', '</w>'),\n",
       " ('lowe', 'st</w>'),\n",
       " ('newe', 'r</w>')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.merge_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3bc54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T09:38:26.536202Z",
     "start_time": "2025-04-01T09:38:26.377571Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dda862511776b696c2274cfbcc32738",
     "grade": true,
     "grade_id": "test_BytePairEncoding_F0",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90a8b85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f83e5f026272925b93ff1e387df31685",
     "grade": false,
     "grade_id": "BytePairEncoding_G_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding F) [5 points]\n",
    "\n",
    "Use your BPE tokenizer on sentences from YELP reviews. Then encode a random sentence using the tokenizer. Finally decode the sentence again.\n",
    "\n",
    "**Note: The encode method of the tokenizer expects a list of sentences (list of lists).**\n",
    "\n",
    "Training might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de503a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:36:15.381438Z",
     "start_time": "2025-04-13T11:36:15.197068Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99a8fe55cb5a12726e3c9ab18aebd105",
     "grade": true,
     "grade_id": "BytePairEncoding_G",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded:\n",
      " [['O', 'r', 'd', 'e', 'r', 'e', 'd', '</w>', 'o', 'n', '</w>', 'R', 'i', 't', 'u', 'a', 'l', '</w>', 'b', 'u', 't', '</w>', 'w', 'a', 's', '</w>', 'n', 'o', 't', '</w>', 'r', 'd', 'y', '</w>', 'w', 'h', 'e', 'n', '</w>', 'i', '</w>', 'w', 'e', 'n', 't', '</w>', 't', 'h', 'e', 'r', 'e', ',', '</w>', 't', 'h', 'e</w>', 'w', 'o', 'r', 'k', 'e', 'r', '</w>', 'd', 'o', 'e', 's', 'n', \"'\", 't', '</w>', 's', 'e', 'e', 'm', '</w>', 't', 'o', '</w>', 'b', 'e</w>', 'w', 'e', 'l', 'l', '</w>', 't', 'r', 'a', 'i', 'n', 'e', 'd', '</w>', 'a', 'n', 'd', '</w>', 'i', 't', '</w>', 'w', 'a', 's', '</w>', 'o', 'b', 'v', 'i', 'o', 'u', 's', 'l', 'y', '</w>', 't', 'o', 'o', '</w>', 'b', 'u', 's', 'y', '</w>', 'f', 'o', 'r', '</w>', 't', 'h', 'e</w>', 'o', 'n', 'e</w>', 'w', 'o', 'r', 'k', 'e', 'r', '.', '</w>'], ['S', 'o', '</w>', 'n', 'o', 't', '</w>', 'r', 'e', 'a', 'l', 'l', 'y', '</w>', 't', 'h', 'e</w>', 'e', 'm', 'p', 'l', 'o', 'y', 'e', 'e', \"'\", 's', '</w>', 'f', 'a', 'u', 'l', 't', '.', '</w>'], ['O', 'n', 't', 'o', '</w>', 't', 'h', 'e</w>', 'f', 'o', 'o', 'd', ',', '</w>', '$', '1', '5', '</w>', 'f', 'o', 'r', '</w>', 'a', '</w>', 'p', 'l', 'a', 't', 'e', '.', '.', '.', '</w>', 'A', 'n', 'd', '</w>', 'i', 't', '</w>', 'w', 'a', 's', 'n', 't', '</w>', 'e', 'v', 'e', 'n', '</w>', 't', 'h', 'a', 't', '</w>', 'm', 'u', 'c', 'h', '</w>', 'f', 'o', 'o', 'd', '.', '</w>']]\n",
      "Decoded:\n",
      " [['Ordered', 'on', 'Ritual', 'but', 'was', 'not', 'rdy', 'when', 'i', 'went', 'there,', 'the', 'worker', \"doesn't\", 'seem', 'to', 'be', 'well', 'trained', 'and', 'it', 'was', 'obviously', 'too', 'busy', 'for', 'the', 'one', 'worker.'], ['So', 'not', 'really', 'the', \"employee's\", 'fault.'], ['Onto', 'the', 'food,', '$15', 'for', 'a', 'plate...', 'And', 'it', 'wasnt', 'even', 'that', 'much', 'food.']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/srv/shares/NLP/datasets/yelp/reviews_sents.txt\", \"r\") as f:\n",
    "    sentences = f.read().split(\"\\n\")\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(sentences, 20)\n",
    "\n",
    "corpus = sentences[:3]\n",
    "encoded = tokenizer.encode(corpus)\n",
    "print(\"Encoded:\\n\", encoded)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded:\\n\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c9289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
